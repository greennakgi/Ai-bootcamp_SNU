{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr35Es081ziN"
      },
      "source": [
        "# | HW1 | Practice 4 Model(MobileNet V2 SSD) 개선\n",
        "\n",
        "Practice 4 실습파일에 제공된 모델은 Detection layer 5개, 3가지 종류의 Aspect ratio를 가진 Anchor box로 구성된다.\n",
        "Model의 Detection 성능을 향상시키기 위해 모델에 아래 다섯가지 사항을 반영하라.\n",
        "\n",
        "**Due: 4/11, 11:59 PM**\n",
        "        \n",
        "**제출** : 다섯가지 사항을 반영한 결과를 **보고서**에 작성후 과제를 수행한 **Jupyter Notebook**과 함께 **\"HW1_학번_이름.zip\"** 형태로 제출하여라.\n",
        "- **최종 제출물 형태 예시** : HW1_2024_12345_jiseojin.zip\n",
        "    - **jupyter notebook**\n",
        "        - 예시 : HW1_2024_12345_jiseojin.ipynb\n",
        "        - output 지우지 않고 제출\n",
        "    - **보고서(pdf)**\n",
        "        - 예시 : HW1_2024_12345_jiseojin.pdf\n",
        "        - 아래 내용을 포함 필수\n",
        "            - 원본 실습 코드에서 어느 부분을 어떻게 수정하였는지에 대한 설명\n",
        "            - 수정된 모델을 Training하고 최고 성능을(mAP)를 기록"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__iRjiNw1ziP"
      },
      "source": [
        "### 1. Detection Layer 추가 (30%)\n",
        "- Detection Layer 1개 추가\n",
        "    - Layer width = 28인 Detection Layer를 3번째 Bottleneck Block 뒤에 추가하여라.\n",
        "\n",
        "### 2. Aspect Ratio 추가 (30%)\n",
        "Anchor box의 aspect ratio가 1, 2, 1/2만 사용되고 있다. SSD 논문에서 사용한 Aspect ratio들을 활용하기 위해 아래의 변화를 반영해 보자.\n",
        "- Aspect ratio가 1인 detection layer에 대하여 현재 Layer의 s값과 다음 Layer의 s값을 곱한뒤 제곱근을 취한 값을 곱해주도록 변경하여라.\n",
        "    - 힌트: aspect ratio 1인 박스는 2개다, SSDInputGenerator과 AnchorBox를 둘다 수정 해야한다\n",
        "- 6개의 Detection layer가 있는 상태에서, (2,3,4)번째 Layer에 대하여만 aspect ratio=3, 1/3을 추가한다.  \n",
        "- SSD 논문에서 해당 내용은 아래와 같다.\n",
        "![SSD_AnchorBox_Explanation.PNG](attachment:SSD_AnchorBox_Explanation.PNG)\n",
        "\n",
        "### 3. Loss 함수 변경 (20%)\n",
        "현재 Localization loss는 L2 loss가 사용되고 있다. 이를 Smooth L1 Loss(Huber Loss)로 바꾸어 적용하여라.\n",
        "- Smooth L1 Loss는 아래와 같다. (delta=1을 적용)\n",
        "    ![Huber%20Loss.PNG](attachment:Huber%20Loss.PNG)\n",
        "- Huber Loss는 직접 구현 하여라 (tf.keras.losses.Huber 와 같이 외부 라이브러리 사용시 0점)\n",
        "\n",
        "### 4. Data Augmentation (10%)\n",
        "- 이미지에 Random horizontal flip을 적용하여라.\n",
        "- **중요** Canvas 전체를 flip하지 말고, CIFAR10 이미지를 flip한 후, canvas에 랜덤하게 배치한다.\n",
        "\n",
        "### 5. mAP 성능 (10%)\n",
        "- IoU=0.5:0.95기준 0.3 이상 달성하여라"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64GzuwLp1ziQ"
      },
      "source": [
        "## <과제 유의사항>\n",
        "조교가 트레이닝을 수행해본 결과 트레이닝 중에 mAP의 등락이 있는 것이 발견되었다. Data의 특성에 의한 것으로 판단되니, 이러한 현상이 발견되더라도 당황하지 말고 트레이닝을 진행해도 된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itr9nkp7evGs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import socket\n",
        "import pickle\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "import struct\n",
        "from tqdm import tqdm\n",
        "from sys import getsizeof\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Add, ReLU, Input, Dense, Dropout, Activation, Flatten \\\n",
        "    , Conv2D, MaxPooling2D, InputLayer, Reshape, DepthwiseConv2D, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "from tensorflow.python.keras import backend\n",
        "from tensorflow.python.keras.utils import layer_utils\n",
        "from tensorflow.keras.applications import imagenet_utils # Updated import statement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q7bAOvViAqm"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "n_classes = 10\n",
        "pos_iou_threshold = 0.3\n",
        "neg_iou_threshold = 0.3\n",
        "score_threshold = 0.01\n",
        "layer_width=[28,14,7,4,2,1]\n",
        "num_boxes = [3,5,5,5,3,3]\n",
        "aspect_ratio = []\n",
        "s_max = 0.9\n",
        "s_min = 0.5\n",
        "batch_size = 16\n",
        "log_dir = './'\n",
        "model_name = 'mobilenetSSD'\n",
        "model_csv_path  = os.path.join(log_dir, (model_name + '.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3rwid3Mitl_"
      },
      "outputs": [],
      "source": [
        "#Load data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "               'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "\n",
        "train_size = x_train.shape[0]\n",
        "test_size = x_test.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4x10Blqiuc3"
      },
      "outputs": [],
      "source": [
        "from utils import calc_iou, match_bipartite_greedy, match_multi, convert_coord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-X2Itc3cMHC"
      },
      "outputs": [],
      "source": [
        "data_augmentation= tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip(mode='horizontal')\n",
        "])\n",
        "\n",
        "augmented_images = data_augmentation(x_train)\n",
        "\n",
        "augmented_images_np = np.array(augmented_images)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESXjVYspjA-x"
      },
      "outputs": [],
      "source": [
        "class SSDInputEncodingGenerator(keras.utils.Sequence):\n",
        "    def __init__(self,\n",
        "                 img_height,\n",
        "                 img_width,\n",
        "                 layer_width,\n",
        "                 n_classes,\n",
        "                 num_boxes,\n",
        "                 s_max,\n",
        "                 s_min,\n",
        "                 aspect_ratio,\n",
        "                 pos_iou_threshold,\n",
        "                 neg_iou_threshold,\n",
        "                background_id,\n",
        "                 images,\n",
        "                 labels,\n",
        "                 data_size,\n",
        "                batch_size=32):\n",
        "        #Consider Background class\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.n_class_withbg = n_classes + 1  #Add background class\n",
        "        self.num_boxes = num_boxes  #List of number of boxes in each classifier layer\n",
        "        self.s_max = s_max # Largest scale of default box\n",
        "        self.s_min = s_min # Smallest scale of default box\n",
        "        self.aspect_ratio = aspect_ratio # List of aspect ratios\n",
        "        self.layer_width = layer_width\n",
        "        self.pos_iou_threshold = pos_iou_threshold\n",
        "        self.neg_iou_threshold = neg_iou_threshold\n",
        "        self.background_id = background_id\n",
        "        self.batch_size=batch_size\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.shuffle = False\n",
        "        self.data_size = data_size\n",
        "\n",
        "        self.xmin_random = np.random.randint(self.img_height - 64, size=[self.data_size])\n",
        "        self.ymin_random = np.random.randint(self.img_height - 64, size=[self.data_size])\n",
        "\n",
        "        self.on_epoch_end()\n",
        "        self.ar_list_component= self.update_aspect_ratios(self.num_boxes)\n",
        "\n",
        "    def update_aspect_ratios(self,num_boxes):\n",
        "        for num in self.num_boxes:\n",
        "            if num == 3:\n",
        "                self.aspect_ratio.append([1, 2, 1/2])\n",
        "            elif num == 5:\n",
        "                self.aspect_ratio.append([1, 2, 1/2, 3, 1/3])\n",
        "        ar_list_component=self.aspect_ratio\n",
        "        return ar_list_component\n",
        "\n",
        "\n",
        "\n",
        "    def convert_image(self, image, label, indexes):\n",
        "        \"\"\"\n",
        "        Convert classification data to object detection data\n",
        "        Randomly locate image in the middle of black canvas\n",
        "        Input\n",
        "            x: Image, shape: (batch_size, image size, image size, #channels)\n",
        "            y: label, shape: (batch_size, )\n",
        "        output\n",
        "            out_x: Image located in the random location of black canvas, shape: (batch_size, canvas size, canvas size, 3)\n",
        "            out_y: label and location of corners(xmin,ymin,xmax,ymax), shape: (batch_size, #objects per image, 1+4) #Objects per image = 1\n",
        "\n",
        "        \"\"\"\n",
        "        orig_image_size = 64\n",
        "        channels = image.shape[-1]\n",
        "\n",
        "        #prepare black canvas\n",
        "        canvas = np.zeros((self.batch_size, self.img_height, self.img_width, channels), np.int64)\n",
        "        out_y = np.zeros((self.batch_size, 1, 5))\n",
        "\n",
        "        xmin = self.xmin_random[indexes]\n",
        "        ymin = self.ymin_random[indexes]\n",
        "        xmax = xmin + orig_image_size\n",
        "        ymax = ymin + orig_image_size\n",
        "\n",
        "\n",
        "        resized = np.zeros((orig_image_size, orig_image_size, 3))\n",
        "        for i in range(batch_size):\n",
        "            resized = cv2.resize(image[i], dsize=(orig_image_size, orig_image_size))\n",
        "            canvas[i, xmin[i]:xmax[i], ymin[i]:ymax[i], :] = resized\n",
        "\n",
        "        out_y[:, 0,0] = label[:,0]\n",
        "        out_y[:, 0, -4:] = np.column_stack([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        return canvas, out_y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        Generate one batch of data\n",
        "        '''\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(indexes)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        '''\n",
        "        Updates indexes after each epoch\n",
        "        '''\n",
        "        self.indexes = np.arange(self.data_size)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(self.images.shape[0] / self.batch_size))\n",
        "\n",
        "    def __data_generation(self, indexes):\n",
        "        \"\"\"\n",
        "        Input: ground truth label,shape: (batch_size, #object per image, 1 + 4)\n",
        "        Output: y_encoded, shape: (batch_size, sum of grid size of all classifier * num_boxes, n_class_withbg + 4 + 4)\n",
        "        1. Create y_encoded template: (B, num_boxes, class + 4 + 4) 4 for gt coordinates and 4 for anchor boxes\n",
        "        2. For each ground truth, calculate iou of gt and anchor boxes\n",
        "        3. Find the highest matching anchor box per each gt and fill in y_encoded template\n",
        "        4. Multi object matching\n",
        "        5. Apply negative iou threshold\n",
        "        6. Transform into Delta format\n",
        "        \"\"\"\n",
        "\n",
        "        images, gt_label = self.convert_image(self.images[indexes], self.labels[indexes], indexes)\n",
        "\n",
        "        # Make class vector to one hot format\n",
        "        class_vector = np.eye(self.n_class_withbg)\n",
        "\n",
        "        #layer_width=[14,7,4,2,1]\n",
        "        for iw in range(len(layer_width)):\n",
        "            # s_max = 0.9  s_min = 0.5\n",
        "            s = s_min + (s_max - s_min) / (len(layer_width) - 1) * (len(layer_width) - iw - 1)\n",
        "            l = layer_width[iw]\n",
        "            num_box = self.num_boxes[iw]       # num_boxes = [3,3,3,3,3]\n",
        "            box_tensor = np.zeros((l * l * num_box, 4))\n",
        "            ar_list= self.ar_list_component[iw]\n",
        "\n",
        "            for i in range(l):\n",
        "                for j in range(l):\n",
        "\n",
        "                        for box_idx in range(num_box):\n",
        "                            box_tensor[(i * l + j) * num_box + box_idx, 0] = (0.5 + i) / l\n",
        "                            box_tensor[(i * l + j) * num_box + box_idx, 1] = (0.5 + j) / l\n",
        "\n",
        "                            # Aspect ratio calculation\n",
        "                            ar = ar_list[box_idx]\n",
        "                            if ar == 1:\n",
        "                                if iw < len(layer_width) - 1:\n",
        "                                    s_next = s_min + (s_max - s_min) / (len(layer_width) - 1) * (len(layer_width) - iw - 2)\n",
        "                                    adjusted_s = math.sqrt(s * s_next)\n",
        "                                else:\n",
        "                                    adjusted_s = s\n",
        "                                box_tensor[(i * l + j) * num_box + box_idx, 2] = adjusted_s / l\n",
        "                                box_tensor[(i * l + j) * num_box + box_idx, 3] = adjusted_s / l\n",
        "                            else:\n",
        "                                box_tensor[(i * l + j) * num_box + box_idx, 2] = s * math.sqrt(ar) / l\n",
        "                                box_tensor[(i * l + j) * num_box + box_idx, 3] = s / math.sqrt(ar) / l\n",
        "\n",
        "\n",
        "            ### 실습 끝\n",
        "\n",
        "            box_tensor = convert_coord(box_tensor, type='centroid2corner')\n",
        "\n",
        "            if iw == 0:\n",
        "                boxes_tensor = box_tensor\n",
        "            else:\n",
        "                boxes_tensor = np.concatenate((boxes_tensor, box_tensor), axis = 0)\n",
        "\n",
        "            class_tensor = np.zeros((l * l * num_box , self.n_class_withbg))\n",
        "\n",
        "            if iw == 0:\n",
        "                classes_tensor = class_tensor\n",
        "            else:\n",
        "                classes_tensor = np.concatenate((classes_tensor, class_tensor), axis = 0)\n",
        "\n",
        "        box_class_tensor= np.concatenate((classes_tensor, boxes_tensor, boxes_tensor), axis = 1)\n",
        "        y_encoded = np.tile(box_class_tensor, (self.batch_size, 1, 1))\n",
        "\n",
        "        y_encoded[:, :, self.background_id] = 1 # All boxes are background boxes by default.\n",
        "\n",
        "\n",
        "        #Ground truth matching\n",
        "        for i in range(self.batch_size):\n",
        "            gt_one_label = gt_label[i]\n",
        "            m = gt_one_label.shape[0]\n",
        "            if gt_one_label.shape[0] == 0: continue # If there is no object, skip\n",
        "\n",
        "            #Normalize ground truth\n",
        "            gt_one_label[:,[-4,-2]] /= self.img_width\n",
        "            gt_one_label[:,[-3,-1]] /= self.img_height\n",
        "\n",
        "            #FInd the iou of ground truth and all anchor boxes\n",
        "            similarities = calc_iou(gt_one_label[:,-4:], y_encoded[i, :, -4:])\n",
        "\n",
        "            #Find the highest matching anchor box per each ground truth boxes\n",
        "            matches = match_bipartite_greedy(similarities)\n",
        "\n",
        "            #Convert ground truth class label to one hot encoding\n",
        "            gt_class = np.array(gt_one_label[:,0] + 1, np.int64)\n",
        "\n",
        "            #Fill in y_encoded\n",
        "            y_encoded[i, matches, :self.n_class_withbg] = class_vector[gt_class]\n",
        "            y_encoded[i, matches, -8:-4] = gt_one_label[:,1:]\n",
        "\n",
        "            #Set the matched anchor boxes to 0 to indicate they are matched before multi object matching\n",
        "            similarities[:,matches] = 0\n",
        "\n",
        "\n",
        "            #Multi object matching\n",
        "            #Similar process to bipartite matching\n",
        "            matches_anchor, matches_gt = match_multi(similarities, threshold=self.pos_iou_threshold)\n",
        "\n",
        "            if len(matches_gt) > 0:\n",
        "\n",
        "                y_encoded[i, matches_anchor, :self.n_class_withbg] = class_vector[gt_class[matches_gt]]\n",
        "                y_encoded[i, matches_anchor, -8:-4] = gt_one_label[matches_gt,1:]\n",
        "\n",
        "                #Set the matched anchor boxes to 0 to indicate they are matched before applying negative iou threshold\n",
        "                similarities[:,matches_anchor] = 0\n",
        "\n",
        "            #All background boxes whose iou are greater than neg_iou_threshold\n",
        "            # are set to neutral(neither background nor class)\n",
        "            max_bg_similarities = np.amax(similarities, axis = 0)\n",
        "            neutral_boxes = np.nonzero(max_bg_similarities >= self.neg_iou_threshold)[0]\n",
        "            y_encoded[i, neutral_boxes, self.background_id] = 0\n",
        "\n",
        "        #Convert coordinate from corner 2 centroid\n",
        "        y_encoded[:,:,:-4] = convert_coord(y_encoded[:,:,:-4], type='corner2centroid')\n",
        "        #print(y_encoded[0,0])\n",
        "        y_encoded = convert_coord(y_encoded, type='corner2centroid')\n",
        "        #print(y_encoded[0,0])\n",
        "\n",
        "        y_encoded[:,:,[-8, -7]] -= y_encoded[:,:,[-4, -3]] # (cx(gt) - cx(d_box)) # (cy(gt) - cy(d_box))\n",
        "        y_encoded[:,:,[-8, -7]] /= y_encoded[:,:,[-2, -1]] # (cx(gt) - cx(d_box)) / w(d_box) # (cy(gt) - cy(d_box)) / h(d_box)\n",
        "        y_encoded[:,:,[-6, -5]] = np.log(y_encoded[:,:,[-6, -5]] / y_encoded[:,:,[-2, -1]]) #log(w(gt) / w(d_box)) #log(h(gt) / h(d_box))\n",
        "\n",
        "        return images, y_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcmABGvfjHPS"
      },
      "outputs": [],
      "source": [
        "ssd_input_gen = SSDInputEncodingGenerator(IMG_SIZE,\n",
        "                 IMG_SIZE,\n",
        "                 layer_width=layer_width,\n",
        "                 n_classes=n_classes,\n",
        "                 num_boxes=num_boxes,\n",
        "                 s_max=s_max,\n",
        "                 s_min=s_min,\n",
        "                 aspect_ratio=aspect_ratio,\n",
        "                 pos_iou_threshold=pos_iou_threshold,\n",
        "                 neg_iou_threshold=neg_iou_threshold,\n",
        "                 background_id=0,\n",
        "                 images=augmented_images_np,\n",
        "                 labels=y_train,\n",
        "                data_size=train_size,\n",
        "                batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaYj210DjI89"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as patches\n",
        "\n",
        "def show(image, label, img_width, img_height):\n",
        "\n",
        "    fig,ax = plt.subplots(1, figsize=(10,10))\n",
        "    ax.imshow(image)\n",
        "    gt_boxes = np.argwhere(label[:,0]==0)\n",
        "\n",
        "    for match in gt_boxes:\n",
        "        anchor_box = label[match[0],-4:]\n",
        "        gt_box = label[match[0],-8:-4]\n",
        "        xmin = anchor_box[0] - anchor_box[2]/2\n",
        "        ymin = anchor_box[1] - anchor_box[3]/2\n",
        "        w = anchor_box[2]\n",
        "        h = anchor_box[3]\n",
        "\n",
        "        w_gt = math.exp(gt_box[2]) * anchor_box[2] * img_width\n",
        "        h_gt = math.exp(gt_box[3]) * anchor_box[3] * img_width\n",
        "        cx_gt = (gt_box[0] * anchor_box[2] + anchor_box[0]) * img_width\n",
        "        cy_gt = (gt_box[1] * anchor_box[3] + anchor_box[1]) * img_width\n",
        "        xmin_gt = (cx_gt - w_gt/2)\n",
        "        ymin_gt = (cy_gt - h_gt/2)\n",
        "\n",
        "        rect = patches.Rectangle((ymin_gt,xmin_gt),h_gt,w_gt,linewidth=1,edgecolor='g',facecolor='none')\n",
        "        ax.text(ymin_gt+1, xmin_gt+5, 'Ground truth box', color='g')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        xmin *= img_width\n",
        "        ymin *= img_height\n",
        "        w *= img_width\n",
        "        h *= img_height\n",
        "\n",
        "        rect = patches.Rectangle((ymin,xmin),h,w,linewidth=1,edgecolor='b',facecolor='none')\n",
        "        ax.text(ymin+1, xmin+5, 'Matched anchor box: {}'.format(match[0]), color='b')\n",
        "        ax.add_patch(rect)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "1Y6mM_2gEXbU",
        "outputId": "9e57d34b-7faa-47bb-c4ac-057e00e0b4a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.25.2\n",
            "Uninstalling numpy-1.25.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/f2py3\n",
            "    /usr/local/bin/f2py3.10\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy-1.25.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-5007b62f.3.23.dev.so\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled numpy-1.25.2\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "b22f3abe4e6e461b9a224f521610dfca"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip uninstall numpy\n",
        "\n",
        "!pip install numpy==1.23.5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Uej6GfOJT7O",
        "outputId": "5fd4c612-4fa8-4072-da06-710bc535f003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.23.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uICp3Rj9jLfK"
      },
      "outputs": [],
      "source": [
        "image, label = next(iter(ssd_input_gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "I67ty214jNWG",
        "outputId": "1cf33b5b-1827-499e-e093-7d61f2091a58"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show(image[4], label[4], IMG_SIZE, IMG_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5trH3gyBfF9v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8k2R9CJjPcu"
      },
      "outputs": [],
      "source": [
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor) # 더 가까운 divisor의 배수로 올림 4,5,6,7,8,9,10,11 -> 8, 12~19 -> 16 ...\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NzCLd5wjRGF"
      },
      "outputs": [],
      "source": [
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    #Get the channel axis and the input channel size\n",
        "    channel_axis = 1 if backend.image_data_format() == 'channels_first' else -1\n",
        "    in_channels = backend.int_shape(inputs)[channel_axis]\n",
        "\n",
        "    pointwise_conv_filters = int(alpha * filters)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8) # Make sure the output filter size is the multiple of 8\n",
        "\n",
        "    #Set the prefix\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    #Expansion block\n",
        "    if block_id: # No expansion for block 0\n",
        "        x = layers.Conv2D(filters = expansion * in_channels, kernel_size = 1, strides = 1, padding='same',\n",
        "                          use_bias=False, activation=None, kernel_regularizer=regularizers.l2(0.00004),\n",
        "                          name=prefix + 'expand')(x)\n",
        "        x = layers.BatchNormalization(axis=channel_axis, momentum=0.999, epsilon=0.001,\n",
        "                                      name=prefix + 'expand_BN')(x)\n",
        "        x = layers.ReLU(6, name=prefix + 'expand_relu')(x)\n",
        "    else:\n",
        "        prefix = 'expanded_conv_'\n",
        "\n",
        "\n",
        "    #Depthwise convolution\n",
        "    #if stride == 2:\n",
        "        #Adjust zero paddings for strides, when input hieght and width are odd add (1,1,1,1) padding / when even, add (0,1,0,1)\n",
        "        #x = layers.ZeroPadding2D(padding=correct_pad(x, 3),\n",
        "        #                         name=prefix + 'pad')(x)\n",
        "\n",
        "    x = layers.DepthwiseConv2D(kernel_size = 3, strides = stride,\n",
        "                               #padding='same' if stride == 1 else 'valid',\n",
        "                               padding='same',\n",
        "                               use_bias=False, activation=None, kernel_regularizer=regularizers.l2(0.00004),\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis, momentum=0.999, epsilon=0.001,\n",
        "                                 name=prefix + 'depthwise_BN')(x)\n",
        "    x = layers.ReLU(6, name=prefix + 'relu')(x)\n",
        "\n",
        "    #Pointwise convolution(Bottleneck)\n",
        "    x = layers.Conv2D(filters = pointwise_filters, kernel_size = 1, strides = 1, padding='same',\n",
        "                      use_bias=False, activation=None, kernel_regularizer=regularizers.l2(0.00004),\n",
        "                      name=prefix + 'project')(x)\n",
        "    x = layers.BatchNormalization(axis=channel_axis, momentum=0.999, epsilon=0.001,\n",
        "                                 name=prefix + 'project_BN')(x)\n",
        "\n",
        "    #Inverted residual only when valid(Input size = output_size)\n",
        "    if in_channels == pointwise_filters and stride == 1:\n",
        "        return layers.add([inputs, x])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDKiQOV-jTmV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "\n",
        "class AnchorBoxes(Layer):\n",
        "    def __init__(self, layer_width, n_class_withbg, num_boxes,\n",
        "                 s_max, s_min, aspect_ratio, index, **kwargs):\n",
        "        self.layer_width = layer_width\n",
        "        self.n_class_withbg = n_class_withbg\n",
        "        self.num_boxes = num_boxes\n",
        "        self.s_max = s_max\n",
        "        self.s_min = s_min\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.index = index\n",
        "        super(AnchorBoxes, self).__init__(**kwargs)\n",
        "\n",
        "        self.ar_list_component= self.update_aspect_ratios(self.num_boxes)\n",
        "\n",
        "    def update_aspect_ratios(self,num_boxes):\n",
        "        for num in self.num_boxes:\n",
        "            if num == 3:\n",
        "                self.aspect_ratio.append([1, 2, 1/2])\n",
        "            elif num == 5:\n",
        "                self.aspect_ratio.append([1, 2, 1/2, 3, 1/3])\n",
        "        ar_list_component=self.aspect_ratio\n",
        "        return ar_list_component\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        super(AnchorBoxes, self).build(input_shape)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if K.image_dim_ordering() == 'tf':\n",
        "            batch_size, feature_map_height, feature_map_width, feature_map_channels = input_shape\n",
        "\n",
        "        return (batch_size, feature_map_height*feature_map_width*self.n_boxes, 4)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'layer_width': list(self.layer_width),\n",
        "            'n_class_withbg': self.n_class_withbg,\n",
        "            'num_boxes': self.num_boxes,\n",
        "            's_max': self.s_max,\n",
        "            's_min': self.s_min,\n",
        "            'aspect_ratio': list(self.aspect_ratio)\n",
        "        }\n",
        "        base_config = super(AnchorBoxes, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        s = self.s_min + (self.s_max - self.s_min) / (len(self.layer_width) - 1) * (len(self.layer_width) - self.index - 1)\n",
        "        l = self.layer_width[self.index]\n",
        "        num_box = self.num_boxes[self.index]\n",
        "        box_tensor = np.zeros((l * l * num_box, 4))\n",
        "\n",
        "        ar_list= self.ar_list_component[self.index]\n",
        "\n",
        "        for i in range(l):\n",
        "            for j in range(l):\n",
        "                for box_idx in range(num_box):\n",
        "                              box_tensor[(i * l + j) * num_box + box_idx, 0] = (0.5 + i) / l\n",
        "                              box_tensor[(i * l + j) * num_box + box_idx, 1] = (0.5 + j) / l\n",
        "\n",
        "                              # Aspect ratio calculation\n",
        "                              ar = ar_list[box_idx]\n",
        "                              if ar == 1:\n",
        "                                  # Initialize iw as the index of the current layer\n",
        "                                  iw = self.index\n",
        "                                  if iw < len(self.layer_width) - 1: # Use self.layer_width\n",
        "                                      s_next = self.s_min + (self.s_max - self.s_min) / (len(self.layer_width) - 1) * (len(self.layer_width) - iw - 2)\n",
        "                                      adjusted_s = math.sqrt(s * s_next)\n",
        "                                  else:\n",
        "                                      adjusted_s = s\n",
        "                                  box_tensor[(i * l + j) * num_box + box_idx, 2] = adjusted_s / l\n",
        "                                  box_tensor[(i * l + j) * num_box + box_idx, 3] = adjusted_s / l\n",
        "                              else:\n",
        "                                  box_tensor[(i * l + j) * num_box + box_idx, 2] = s * math.sqrt(ar) / l\n",
        "                                  box_tensor[(i * l + j) * num_box + box_idx, 3] = s / math.sqrt(ar) / l\n",
        "\n",
        "        box_tensor = np.expand_dims(box_tensor, axis = 0)\n",
        "        return tf.tile(tf.constant(box_tensor, dtype=tf.float32), (tf.shape(x)[0],1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiRMvxdBjV6h"
      },
      "outputs": [],
      "source": [
        "def MobileNetV2SSD(input_shape,\n",
        "                n_classes,\n",
        "                layer_width,\n",
        "                num_boxes,\n",
        "                alpha=1.0):\n",
        "\n",
        "    n_class_withbg = n_classes + 1 # Add background class\n",
        "\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    first_block_filters = _make_divisible(32 * alpha, 8)\n",
        "    # first conv layer: 224x224x3 -> 112x112x32\n",
        "    x = layers.Conv2D(first_block_filters, kernel_size=3, strides=(2, 2), padding='same',  use_bias=False,\n",
        "                      bias_initializer='zeros',  kernel_regularizer=regularizers.l2(0.00004),\n",
        "                      name='Conv1')(inputs)\n",
        "\n",
        "    x = layers.BatchNormalization(\n",
        "      axis=-1, epsilon=1e-3, momentum=0.999, name='bn_Conv1')(x)\n",
        "\n",
        "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
        "\n",
        "\n",
        "    # inverted residual blocks\n",
        "    # 1st bottleneck block: 112x112x32 -> 112x112x16\n",
        "    x = _inverted_res_block(\n",
        "      x, filters=16, alpha=alpha, stride=1, expansion=1, block_id=0)\n",
        "\n",
        "    # 2nd bottleneck block: 112x112x16 -> 56x56x24\n",
        "    x = _inverted_res_block(\n",
        "      x, filters=24, alpha=alpha, stride=2, expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(\n",
        "      x, filters=24, alpha=alpha, stride=1, expansion=6, block_id=2)\n",
        "\n",
        "    # 3rd bottleneck block: 56x56x24 -> 28x28x32\n",
        "    x = _inverted_res_block(\n",
        "      x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(\n",
        "      x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(\n",
        "      x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=5)\n",
        "\n",
        "    classifier_1_conf = layers.Conv2D(num_boxes[0] * n_class_withbg, kernel_size = 3, padding='same', use_bias=False, name='classifier_1_conf')(x)\n",
        "    classifier_1_loc = layers.Conv2D(num_boxes[0] * 4, kernel_size = 3, padding='same', use_bias=False, name='classifier_1_loc')(x)\n",
        "\n",
        "    # 4th bottleneck block: 28x28x32 -> 14x14x64\n",
        "    x = _inverted_res_block(\n",
        "      x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=6)\n",
        "    x = _inverted_res_block(\n",
        "      x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=7)\n",
        "    x = _inverted_res_block(\n",
        "      x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=8)\n",
        "    x = _inverted_res_block(\n",
        "      x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=9)\n",
        "\n",
        "    classifier_2_conf = layers.Conv2D(num_boxes[1] * n_class_withbg, kernel_size = 3, padding='same', use_bias=False, name='classifier_2_conf')(x)\n",
        "    classifier_2_loc = layers.Conv2D(num_boxes[1] * 4, kernel_size = 3, padding='same', use_bias=False, name='classifier_2_loc')(x)\n",
        "\n",
        "\n",
        "    x = layers.Conv2D(256, kernel_size=1, padding='same', use_bias=False, activation='relu')(x)\n",
        "    x = layers.Conv2D(512, kernel_size=3, strides=2, padding='same', use_bias=False, activation='relu')(x) #7x7\n",
        "\n",
        "    classifier_3_conf = layers.Conv2D(num_boxes[2] * n_class_withbg, kernel_size = 3, padding='same', use_bias=False, name='classifier_3_conf')(x)\n",
        "    classifier_3_loc = layers.Conv2D(num_boxes[2] * 4, kernel_size = 3, padding='same', use_bias=False, name='classifier_3_loc')(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=1, padding='same', use_bias=False, activation='relu')(x)\n",
        "    x = layers.Conv2D(256, kernel_size=3, strides=2, padding='same', use_bias=False, activation='relu')(x) # 4x4\n",
        "\n",
        "    classifier_4_conf = layers.Conv2D(num_boxes[3] * n_class_withbg, kernel_size = 3, padding='same', use_bias=False, name='classifier_4_conf')(x)\n",
        "    classifier_4_loc = layers.Conv2D(num_boxes[3] * 4, kernel_size = 3, padding='same', use_bias=False, name='classifier_4_loc')(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=1, padding='same', use_bias=False, activation='relu')(x)\n",
        "    x = layers.Conv2D(256, kernel_size=3, strides=2, padding='same', use_bias=False, activation='relu')(x) # 2x2\n",
        "\n",
        "    classifier_5_conf = layers.Conv2D(num_boxes[4] * n_class_withbg, kernel_size = 3, padding='same', use_bias=False, name='classifier_5_conf')(x)\n",
        "    classifier_5_loc = layers.Conv2D(num_boxes[4] * 4, kernel_size = 3, padding='same', use_bias=False, name='classifier_5_loc')(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=1, padding='same', use_bias=False, activation='relu')(x)\n",
        "    x = layers.Conv2D(256, kernel_size=3, strides=2, padding='same', use_bias=False, activation='relu')(x) # 1x1\n",
        "\n",
        "    classifier_6_conf = layers.Conv2D(num_boxes[5] * n_class_withbg, kernel_size = 3, padding='same', use_bias=False, name='classifier_6_conf')(x)\n",
        "    classifier_6_loc = layers.Conv2D(num_boxes[5] * 4, kernel_size = 3, padding='same', use_bias=False, name='classifier_6_loc')(x)\n",
        "\n",
        "\n",
        "    ### 아래 실습하면서 완성\n",
        "    #Classification tensors\n",
        "    classifier_1_conf = layers.Reshape((layer_width[0] * layer_width[0] * num_boxes[0], n_class_withbg))(classifier_1_conf)\n",
        "    classifier_2_conf = layers.Reshape((layer_width[1] * layer_width[1] * num_boxes[1], n_class_withbg))(classifier_2_conf)\n",
        "    classifier_3_conf = layers.Reshape((layer_width[2]*layer_width[2]*num_boxes[2], n_class_withbg))(classifier_3_conf)\n",
        "    classifier_4_conf = layers.Reshape((layer_width[3]*layer_width[3]*num_boxes[3], n_class_withbg))(classifier_4_conf)\n",
        "    classifier_5_conf = layers.Reshape((layer_width[4]*layer_width[4]*num_boxes[4], n_class_withbg))(classifier_5_conf)\n",
        "    classifier_6_conf = layers.Reshape((layer_width[5]*layer_width[5]*num_boxes[5], n_class_withbg))(classifier_6_conf)\n",
        "    conf_layers = layers.concatenate([classifier_1_conf, classifier_2_conf, classifier_3_conf, classifier_4_conf, classifier_5_conf,classifier_6_conf], axis = 1)\n",
        "\n",
        "\n",
        "    #Apply softmax\n",
        "    conf_layers_softmax = layers.Activation('softmax')(conf_layers)\n",
        "\n",
        "    #Localization tensors\n",
        "    classifier_1_loc = layers.Reshape((layer_width[0] * layer_width[0] * num_boxes[0], 4))(classifier_1_loc)\n",
        "    classifier_2_loc = layers.Reshape((layer_width[1] * layer_width[1] * num_boxes[1], 4))(classifier_2_loc)\n",
        "    classifier_3_loc = layers.Reshape((layer_width[2]*layer_width[2]*num_boxes[2], 4))(classifier_3_loc)\n",
        "    classifier_4_loc = layers.Reshape((layer_width[3]*layer_width[3]*num_boxes[3], 4))(classifier_4_loc)\n",
        "    classifier_5_loc = layers.Reshape((layer_width[4]*layer_width[4]*num_boxes[4], 4))(classifier_5_loc)\n",
        "    classifier_6_loc = layers.Reshape((layer_width[5]*layer_width[5]*num_boxes[5], 4))(classifier_6_loc)\n",
        "\n",
        "    loc_layers = layers.concatenate([classifier_1_loc, classifier_2_loc, classifier_3_loc, classifier_4_loc, classifier_5_loc,classifier_6_loc], axis = 1)\n",
        "\n",
        "    #Default anchor box tensors, They are constant and NOT trained !!\n",
        "    #def __init__(self, layer_width, n_class_withbg, num_boxes, s_max, s_min, aspect_ratio, index, **kwargs):\n",
        "    dbox_1 = AnchorBoxes(layer_width, n_class_withbg, num_boxes, s_max, s_min, aspect_ratio, index = 0)(classifier_1_loc)\n",
        "    dbox_2 = AnchorBoxes(layer_width, n_class_withbg, num_boxes, s_max, s_min, aspect_ratio, index = 1)(classifier_2_loc)\n",
        "    dbox_3 = AnchorBoxes(layer_width, n_class_withbg, num_boxes, s_max, s_min, aspect_ratio, index = 2)(classifier_3_loc)\n",
        "    dbox_4 = AnchorBoxes(layer_width, n_class_withbg, num_boxes, s_max, s_min, aspect_ratio, index = 3)(classifier_4_loc)\n",
        "    dbox_5 = AnchorBoxes(layer_width, n_class_withbg, num_boxes, s_max, s_min, aspect_ratio, index = 4)(classifier_5_loc)\n",
        "    dbox_6 = AnchorBoxes(layer_width, n_class_withbg, num_boxes, s_max, s_min, aspect_ratio, index = 5)(classifier_6_loc)\n",
        "\n",
        "    dbox_layers = layers.concatenate([dbox_1, dbox_2, dbox_3, dbox_4, dbox_5, dbox_6], axis = 1)\n",
        "\n",
        "    ### 실습 끝\n",
        "\n",
        "\n",
        "    #Concatenate Classification tensor, Localization tensor and Default anchor box tensor.\n",
        "    detections = layers.concatenate([conf_layers_softmax, loc_layers, dbox_layers], axis=-1)\n",
        "\n",
        "    outputs = detections\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74Te3mbBfGyZ"
      },
      "outputs": [],
      "source": [
        "model = MobileNetV2SSD((IMG_SIZE, IMG_SIZE, 3), n_classes, layer_width, num_boxes, alpha=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufp42yP5RLG1",
        "outputId": "9684b36a-a3dc-4691-92c9-54669fe0deba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " Conv1 (Conv2D)              (None, 112, 112, 32)         864       ['input_7[0][0]']             \n",
            "                                                                                                  \n",
            " bn_Conv1 (BatchNormalizati  (None, 112, 112, 32)         128       ['Conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " Conv1_relu (ReLU)           (None, 112, 112, 32)         0         ['bn_Conv1[0][0]']            \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise (D  (None, 112, 112, 32)         288       ['Conv1_relu[0][0]']          \n",
            " epthwiseConv2D)                                                                                  \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_BN  (None, 112, 112, 32)         128       ['expanded_conv_depthwise[0][0\n",
            "  (BatchNormalization)                                              ]']                           \n",
            "                                                                                                  \n",
            " expanded_conv_relu (ReLU)   (None, 112, 112, 32)         0         ['expanded_conv_depthwise_BN[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " expanded_conv_project (Con  (None, 112, 112, 16)         512       ['expanded_conv_relu[0][0]']  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " expanded_conv_project_BN (  (None, 112, 112, 16)         64        ['expanded_conv_project[0][0]'\n",
            " BatchNormalization)                                                ]                             \n",
            "                                                                                                  \n",
            " block_1_expand (Conv2D)     (None, 112, 112, 96)         1536      ['expanded_conv_project_BN[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " block_1_expand_BN (BatchNo  (None, 112, 112, 96)         384       ['block_1_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_1_expand_relu (ReLU)  (None, 112, 112, 96)         0         ['block_1_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_1_depthwise (Depthwi  (None, 56, 56, 96)           864       ['block_1_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_1_depthwise_BN (Batc  (None, 56, 56, 96)           384       ['block_1_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_1_relu (ReLU)         (None, 56, 56, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
            "                                                                                                  \n",
            " block_1_project (Conv2D)    (None, 56, 56, 24)           2304      ['block_1_relu[0][0]']        \n",
            "                                                                                                  \n",
            " block_1_project_BN (BatchN  (None, 56, 56, 24)           96        ['block_1_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_expand (Conv2D)     (None, 56, 56, 144)          3456      ['block_1_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_2_expand_BN (BatchNo  (None, 56, 56, 144)          576       ['block_2_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_2_expand_relu (ReLU)  (None, 56, 56, 144)          0         ['block_2_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_2_depthwise (Depthwi  (None, 56, 56, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_depthwise_BN (Batc  (None, 56, 56, 144)          576       ['block_2_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_2_relu (ReLU)         (None, 56, 56, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
            "                                                                                                  \n",
            " block_2_project (Conv2D)    (None, 56, 56, 24)           3456      ['block_2_relu[0][0]']        \n",
            "                                                                                                  \n",
            " block_2_project_BN (BatchN  (None, 56, 56, 24)           96        ['block_2_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_36 (Add)                (None, 56, 56, 24)           0         ['block_1_project_BN[0][0]',  \n",
            "                                                                     'block_2_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_3_expand (Conv2D)     (None, 56, 56, 144)          3456      ['add_36[0][0]']              \n",
            "                                                                                                  \n",
            " block_3_expand_BN (BatchNo  (None, 56, 56, 144)          576       ['block_3_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_3_expand_relu (ReLU)  (None, 56, 56, 144)          0         ['block_3_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_3_depthwise (Depthwi  (None, 28, 28, 144)          1296      ['block_3_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_3_depthwise_BN (Batc  (None, 28, 28, 144)          576       ['block_3_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_3_relu (ReLU)         (None, 28, 28, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
            "                                                                                                  \n",
            " block_3_project (Conv2D)    (None, 28, 28, 32)           4608      ['block_3_relu[0][0]']        \n",
            "                                                                                                  \n",
            " block_3_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_3_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_expand (Conv2D)     (None, 28, 28, 192)          6144      ['block_3_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_4_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_4_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_4_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_4_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_4_depthwise (Depthwi  (None, 28, 28, 192)          1728      ['block_4_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_depthwise_BN (Batc  (None, 28, 28, 192)          768       ['block_4_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_4_relu (ReLU)         (None, 28, 28, 192)          0         ['block_4_depthwise_BN[0][0]']\n",
            "                                                                                                  \n",
            " block_4_project (Conv2D)    (None, 28, 28, 32)           6144      ['block_4_relu[0][0]']        \n",
            "                                                                                                  \n",
            " block_4_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_4_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_37 (Add)                (None, 28, 28, 32)           0         ['block_3_project_BN[0][0]',  \n",
            "                                                                     'block_4_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_5_expand (Conv2D)     (None, 28, 28, 192)          6144      ['add_37[0][0]']              \n",
            "                                                                                                  \n",
            " block_5_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_5_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_5_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_5_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_5_depthwise (Depthwi  (None, 28, 28, 192)          1728      ['block_5_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_5_depthwise_BN (Batc  (None, 28, 28, 192)          768       ['block_5_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_5_relu (ReLU)         (None, 28, 28, 192)          0         ['block_5_depthwise_BN[0][0]']\n",
            "                                                                                                  \n",
            " block_5_project (Conv2D)    (None, 28, 28, 32)           6144      ['block_5_relu[0][0]']        \n",
            "                                                                                                  \n",
            " block_5_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_5_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_38 (Add)                (None, 28, 28, 32)           0         ['add_37[0][0]',              \n",
            "                                                                     'block_5_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_6_expand (Conv2D)     (None, 28, 28, 192)          6144      ['add_38[0][0]']              \n",
            "                                                                                                  \n",
            " block_6_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_6_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_6_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_6_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_6_depthwise (Depthwi  (None, 14, 14, 192)          1728      ['block_6_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_6_depthwise_BN (Batc  (None, 14, 14, 192)          768       ['block_6_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_6_relu (ReLU)         (None, 14, 14, 192)          0         ['block_6_depthwise_BN[0][0]']\n",
            "                                                                                                  \n",
            " block_6_project (Conv2D)    (None, 14, 14, 64)           12288     ['block_6_relu[0][0]']        \n",
            "                                                                                                  \n",
            " block_6_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_6_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_7_expand (Conv2D)     (None, 14, 14, 384)          24576     ['block_6_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_7_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_7_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_7_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_7_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_7_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_7_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_7_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_7_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_7_relu (ReLU)         (None, 14, 14, 384)          0         ['block_7_depthwise_BN[0][0]']\n",
            "                                                                                                  \n",
            " block_7_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_7_relu[0][0]']        \n",
            "                                                                                                  \n",
            " block_7_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_7_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_39 (Add)                (None, 14, 14, 64)           0         ['block_6_project_BN[0][0]',  \n",
            "                                                                     'block_7_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_8_expand (Conv2D)     (None, 14, 14, 384)          24576     ['add_39[0][0]']              \n",
            "                                                                                                  \n",
            " block_8_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_8_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_8_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_8_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_8_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_8_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_8_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_8_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_8_relu (ReLU)         (None, 14, 14, 384)          0         ['block_8_depthwise_BN[0][0]']\n",
            "                                                                                                  \n",
            " block_8_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_8_relu[0][0]']        \n",
            "                                                                                                  \n",
            " block_8_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_8_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_40 (Add)                (None, 14, 14, 64)           0         ['add_39[0][0]',              \n",
            "                                                                     'block_8_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_9_expand (Conv2D)     (None, 14, 14, 384)          24576     ['add_40[0][0]']              \n",
            "                                                                                                  \n",
            " block_9_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_9_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_9_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_9_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_9_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_9_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_9_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_9_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_9_relu (ReLU)         (None, 14, 14, 384)          0         ['block_9_depthwise_BN[0][0]']\n",
            "                                                                                                  \n",
            " block_9_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_9_relu[0][0]']        \n",
            "                                                                                                  \n",
            " block_9_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_9_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_41 (Add)                (None, 14, 14, 64)           0         ['add_40[0][0]',              \n",
            "                                                                     'block_9_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)          (None, 14, 14, 256)          16384     ['add_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)          (None, 7, 7, 512)            1179648   ['conv2d_48[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)          (None, 7, 7, 128)            65536     ['conv2d_49[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)          (None, 4, 4, 256)            294912    ['conv2d_50[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)          (None, 4, 4, 128)            32768     ['conv2d_51[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)          (None, 2, 2, 256)            294912    ['conv2d_52[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)          (None, 2, 2, 128)            32768     ['conv2d_53[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)          (None, 1, 1, 256)            294912    ['conv2d_54[0][0]']           \n",
            "                                                                                                  \n",
            " classifier_1_conf (Conv2D)  (None, 28, 28, 33)           9504      ['add_38[0][0]']              \n",
            "                                                                                                  \n",
            " classifier_2_conf (Conv2D)  (None, 14, 14, 55)           31680     ['add_41[0][0]']              \n",
            "                                                                                                  \n",
            " classifier_3_conf (Conv2D)  (None, 7, 7, 55)             253440    ['conv2d_49[0][0]']           \n",
            "                                                                                                  \n",
            " classifier_4_conf (Conv2D)  (None, 4, 4, 55)             126720    ['conv2d_51[0][0]']           \n",
            "                                                                                                  \n",
            " classifier_5_conf (Conv2D)  (None, 2, 2, 33)             76032     ['conv2d_53[0][0]']           \n",
            "                                                                                                  \n",
            " classifier_6_conf (Conv2D)  (None, 1, 1, 33)             76032     ['conv2d_55[0][0]']           \n",
            "                                                                                                  \n",
            " classifier_1_loc (Conv2D)   (None, 28, 28, 12)           3456      ['add_38[0][0]']              \n",
            "                                                                                                  \n",
            " classifier_2_loc (Conv2D)   (None, 14, 14, 20)           11520     ['add_41[0][0]']              \n",
            "                                                                                                  \n",
            " classifier_3_loc (Conv2D)   (None, 7, 7, 20)             92160     ['conv2d_49[0][0]']           \n",
            "                                                                                                  \n",
            " classifier_4_loc (Conv2D)   (None, 4, 4, 20)             46080     ['conv2d_51[0][0]']           \n",
            "                                                                                                  \n",
            " classifier_5_loc (Conv2D)   (None, 2, 2, 12)             27648     ['conv2d_53[0][0]']           \n",
            "                                                                                                  \n",
            " classifier_6_loc (Conv2D)   (None, 1, 1, 12)             27648     ['conv2d_55[0][0]']           \n",
            "                                                                                                  \n",
            " reshape_72 (Reshape)        (None, 2352, 11)             0         ['classifier_1_conf[0][0]']   \n",
            "                                                                                                  \n",
            " reshape_73 (Reshape)        (None, 980, 11)              0         ['classifier_2_conf[0][0]']   \n",
            "                                                                                                  \n",
            " reshape_74 (Reshape)        (None, 245, 11)              0         ['classifier_3_conf[0][0]']   \n",
            "                                                                                                  \n",
            " reshape_75 (Reshape)        (None, 80, 11)               0         ['classifier_4_conf[0][0]']   \n",
            "                                                                                                  \n",
            " reshape_76 (Reshape)        (None, 12, 11)               0         ['classifier_5_conf[0][0]']   \n",
            "                                                                                                  \n",
            " reshape_77 (Reshape)        (None, 3, 11)                0         ['classifier_6_conf[0][0]']   \n",
            "                                                                                                  \n",
            " reshape_78 (Reshape)        (None, 2352, 4)              0         ['classifier_1_loc[0][0]']    \n",
            "                                                                                                  \n",
            " reshape_79 (Reshape)        (None, 980, 4)               0         ['classifier_2_loc[0][0]']    \n",
            "                                                                                                  \n",
            " reshape_80 (Reshape)        (None, 245, 4)               0         ['classifier_3_loc[0][0]']    \n",
            "                                                                                                  \n",
            " reshape_81 (Reshape)        (None, 80, 4)                0         ['classifier_4_loc[0][0]']    \n",
            "                                                                                                  \n",
            " reshape_82 (Reshape)        (None, 12, 4)                0         ['classifier_5_loc[0][0]']    \n",
            "                                                                                                  \n",
            " reshape_83 (Reshape)        (None, 3, 4)                 0         ['classifier_6_loc[0][0]']    \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenat  (None, 3672, 11)             0         ['reshape_72[0][0]',          \n",
            " e)                                                                  'reshape_73[0][0]',          \n",
            "                                                                     'reshape_74[0][0]',          \n",
            "                                                                     'reshape_75[0][0]',          \n",
            "                                                                     'reshape_76[0][0]',          \n",
            "                                                                     'reshape_77[0][0]']          \n",
            "                                                                                                  \n",
            " anchor_boxes_6 (AnchorBoxe  (None, 2352, 4)              0         ['reshape_78[0][0]']          \n",
            " s)                                                                                               \n",
            "                                                                                                  \n",
            " anchor_boxes_7 (AnchorBoxe  (None, 980, 4)               0         ['reshape_79[0][0]']          \n",
            " s)                                                                                               \n",
            "                                                                                                  \n",
            " anchor_boxes_8 (AnchorBoxe  (None, 245, 4)               0         ['reshape_80[0][0]']          \n",
            " s)                                                                                               \n",
            "                                                                                                  \n",
            " anchor_boxes_9 (AnchorBoxe  (None, 80, 4)                0         ['reshape_81[0][0]']          \n",
            " s)                                                                                               \n",
            "                                                                                                  \n",
            " anchor_boxes_10 (AnchorBox  (None, 12, 4)                0         ['reshape_82[0][0]']          \n",
            " es)                                                                                              \n",
            "                                                                                                  \n",
            " anchor_boxes_11 (AnchorBox  (None, 3, 4)                 0         ['reshape_83[0][0]']          \n",
            " es)                                                                                              \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 3672, 11)             0         ['concatenate_12[0][0]']      \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenat  (None, 3672, 4)              0         ['reshape_78[0][0]',          \n",
            " e)                                                                  'reshape_79[0][0]',          \n",
            "                                                                     'reshape_80[0][0]',          \n",
            "                                                                     'reshape_81[0][0]',          \n",
            "                                                                     'reshape_82[0][0]',          \n",
            "                                                                     'reshape_83[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenat  (None, 3672, 4)              0         ['anchor_boxes_6[0][0]',      \n",
            " e)                                                                  'anchor_boxes_7[0][0]',      \n",
            "                                                                     'anchor_boxes_8[0][0]',      \n",
            "                                                                     'anchor_boxes_9[0][0]',      \n",
            "                                                                     'anchor_boxes_10[0][0]',     \n",
            "                                                                     'anchor_boxes_11[0][0]']     \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenat  (None, 3672, 19)             0         ['activation_6[0][0]',        \n",
            " e)                                                                  'concatenate_13[0][0]',      \n",
            "                                                                     'concatenate_14[0][0]']      \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3242528 (12.37 MB)\n",
            "Trainable params: 3233120 (12.33 MB)\n",
            "Non-trainable params: 9408 (36.75 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLj1rhN2RV_W"
      },
      "outputs": [],
      "source": [
        "class SSDLoss():\n",
        "    def __init__(self, n_classes, background_id, neg_pos_ratio=3, n_neg_min=0, alpha=1.0, beta=1.0):\n",
        "        self.neg_pos_ratio = neg_pos_ratio\n",
        "        self.n_neg_min = 0\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.background_id = background_id\n",
        "        self.n_class_withbg = n_classes + 1\n",
        "\n",
        "\n",
        "    def smoothL1Loss(self, loc_true, loc_pred):\n",
        "        \"\"\"\n",
        "        y_true: ground truth localization tensor, shape: (batch_size, num_boxes, 4)\n",
        "        y_pred: predicted localization tensor, shape: (batch_size, num_boxes, 4)\n",
        "        \"\"\"\n",
        "        diff = tf.abs(loc_pred - loc_true)\n",
        "        less_than_one = tf.cast(tf.less(diff, 1.0), tf.float32)\n",
        "        smooth_l1_loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n",
        "        return tf.reduce_sum(smooth_l1_loss, axis=-1)\n",
        "\n",
        "\n",
        "    def log_loss(self, class_true, class_pred):\n",
        "        #classification loss\n",
        "        class_pred = tf.maximum(class_pred, 1e-15)\n",
        "        log_loss = -tf.reduce_sum(class_true * tf.math.log(class_pred), axis=-1)  # (batch_size, n_boxes)\n",
        "        return log_loss\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        y_true: (batch_size, # boxes, n_class_withbg + 4)\n",
        "        y_pred: (batch_size, # boxes, n_class_withbg + 4)\n",
        "        \"\"\"\n",
        "\n",
        "        #Get the size of tensor\n",
        "        batch_size = tf.shape(y_true)[0]\n",
        "        n_boxes = tf.shape(y_pred)[1]\n",
        "\n",
        "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "        classification_loss = self.log_loss(y_true[:, :, :self.n_class_withbg], y_pred[:, :, :self.n_class_withbg])\n",
        "        positives = tf.reduce_max(y_true[:, :, 1:(self.n_class_withbg)], axis=-1) # Class is NOT background, (batch_size, n_boxes)\n",
        "\n",
        "        # Classification loss for positive boxes\n",
        "        pos_class_loss = tf.reduce_sum(self.log_loss(y_true[:, :, 1:self.n_class_withbg], y_pred[:, :, 1:self.n_class_withbg]), axis=-1)\n",
        "\n",
        "        negatives = y_true[:,:, self.background_id] # Class is background, (batch_size, n_boxes)\n",
        "        n_positives = tf.reduce_sum(positives) # number of positive boxes, single number\n",
        "\n",
        "        # Classification loss for negative boxes\n",
        "        neg_class_loss_all = classification_loss * negatives #(batch_size, n_boxes)\n",
        "        n_neg_losses = tf.math.count_nonzero(neg_class_loss_all, dtype=tf.int32) # The number of nonzero entries in neg_class_loss_all\n",
        "\n",
        "        # Keep the number of negative boxes between n_neg_min and neg_pos_ratio * positive_boxes\n",
        "        n_negative_keep = tf.minimum(tf.maximum(self.neg_pos_ratio * tf.cast(n_positives, dtype=tf.int32), self.n_neg_min), n_neg_losses)\n",
        "\n",
        "        def f1():\n",
        "            return tf.zeros([batch_size])\n",
        "\n",
        "        # Get the \"n_negative_keep\" largest confidence negative boxes.\n",
        "        def f2():\n",
        "            #Resahpe neg_class_loss_all to 1d array\n",
        "            neg_class_loss_all_1D = tf.reshape(neg_class_loss_all, [-1])\n",
        "\n",
        "            # Find top 'n_negative_keep' boxes from neg_class_loss_all_1D\n",
        "            values, indices = tf.nn.top_k(neg_class_loss_all_1D, k=n_negative_keep, sorted=False)\n",
        "\n",
        "            #Then create a mask for negative boxes: For selected box above, set them as 1\n",
        "            negatives_keep = tf.scatter_nd(indices=tf.expand_dims(indices, axis=1),\n",
        "                                           updates=tf.ones_like(indices, dtype=tf.int32),\n",
        "                                           shape=tf.shape(neg_class_loss_all_1D))\n",
        "            negatives_keep = tf.cast(tf.reshape(negatives_keep, [batch_size, n_boxes]), dtype=tf.float32)\n",
        "\n",
        "            #Finally compute negative loss\n",
        "            neg_class_loss = tf.reduce_sum(classification_loss * negatives_keep, axis=-1) #(batch_size)\n",
        "\n",
        "            return neg_class_loss\n",
        "\n",
        "        neg_class_loss = tf.cond(tf.equal(n_neg_losses, tf.constant(0)), f1, f2)\n",
        "\n",
        "        #Final classification loss: Sum of positive and negative loss\n",
        "        class_loss = self.beta * pos_class_loss + neg_class_loss\n",
        "\n",
        "        #localization loss\n",
        "        loc_pred = y_pred[:,:,self.n_class_withbg:-4]\n",
        "        loc_true = y_true[:,:,self.n_class_withbg:-4]\n",
        "        loc_loss = self.smoothL1Loss(loc_true, loc_pred) # (batch_size, n_boxes)\n",
        "\n",
        "        # Include only positive boxes in calculating localization loss\n",
        "        loc_loss = tf.reduce_sum(positives * loc_loss, axis=-1) #(batch_size)\n",
        "\n",
        "        #Combine localization and classification loss, divide by matched default box(n_positives)\n",
        "        total_loss = (class_loss + loc_loss * self.alpha) / tf.maximum(1.0, n_positives)\n",
        "\n",
        "        # We divided by n_positives - # of all matched default boxes of \"a batch\"\n",
        "        # Since keras divides by the size of batch, it is double division\n",
        "        # To adjust this, we multiply by batch_size\n",
        "        total_loss = total_loss * tf.cast(batch_size, dtype=tf.float32)\n",
        "\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3PQazMQWgRe"
      },
      "outputs": [],
      "source": [
        "ssd_loss = SSDLoss(n_classes = n_classes, background_id=0, neg_pos_ratio=1, alpha=10, beta=3)\n",
        "model.compile(loss=ssd_loss.compute_loss,\n",
        "              optimizer=tf.keras.optimizers.Adam())\n",
        "              #optimizer=tf.keras.optimizers.SGD(lr=0.001, momentum=0.9, decay=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhFvoh0fWtIE"
      },
      "outputs": [],
      "source": [
        "#decay could be applied using Learning rate scheduler\n",
        "def decay(epoch):\n",
        "    return 0.001 * (0.98 **(epoch - 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTadjx6MWuN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe53a87-48ae-4f7a-f303-e2322df7bb1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint directory /content/drive/MyDrive/training_checkpoints_SSD already exists.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "callbacks = []\n",
        "\"\"\"\n",
        "#TensorBoard로 훈련 성과를 보고 싶은 경우\n",
        "callbacks.append(TensorBoard(log_dir=log_dir, histogram_freq=1))\n",
        "\"\"\"\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/training_checkpoints_SSD/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "    print(f\"Checkpoint directory {checkpoint_dir} created.\")\n",
        "else:\n",
        "    print(f\"Checkpoint directory {checkpoint_dir} already exists.\")\n",
        "\n",
        "# Checkpoint 콜백 설정\n",
        "cp_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    verbose=1,\n",
        "    save_freq='epoch')\n",
        "\n",
        "\n",
        "#Learning rate 스케쥴 설정\n",
        "callbacks.append(LearningRateScheduler(decay))\n",
        "\n",
        "#General logs on csv\n",
        "callbacks.append(CSVLogger(model_csv_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5of6bNlbU4f",
        "outputId": "6bfc9e98-4c94-479d-c562-3e19b574fc89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3125/3125 [==============================] - 7130s 2s/step - loss: 7.5256 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "3125/3125 [==============================] - 7156s 2s/step - loss: 5.9698 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "3125/3125 [==============================] - 7132s 2s/step - loss: 5.4092 - lr: 9.8000e-04\n",
            "Epoch 4/50\n",
            "3125/3125 [==============================] - 7160s 2s/step - loss: 4.9450 - lr: 9.6040e-04\n",
            "Epoch 5/50\n",
            "3125/3125 [==============================] - 7225s 2s/step - loss: 4.5068 - lr: 9.4119e-04\n",
            "Epoch 6/50\n",
            " 815/3125 [======>.......................] - ETA: 1:29:30 - loss: 4.1493"
          ]
        }
      ],
      "source": [
        "history = model.fit(ssd_input_gen,\n",
        "              epochs=50,\n",
        "              verbose=1,\n",
        "              callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3UQAgvSLzsG"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tFoul3olgfz"
      },
      "outputs": [],
      "source": [
        "from utils import decode_detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuiQsGfBgNXn"
      },
      "outputs": [],
      "source": [
        "# Decode the prediction for one image in the image batch\n",
        "image_no = 0\n",
        "y_decoded = decode_detections(np.expand_dims(y_pred[image_no], axis=0),\n",
        "                            n_classes=10,\n",
        "                              confidence_thresh=0.01,\n",
        "                              iou_threshold=0.45,\n",
        "                              top_k=10,\n",
        "                              img_height=IMG_SIZE,\n",
        "                              img_width=IMG_SIZE,\n",
        "                              background_id=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fsp_MgcL4_Q"
      },
      "outputs": [],
      "source": [
        "y_pred[image_no,:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOJy4hfjL6_r"
      },
      "outputs": [],
      "source": [
        "# Visualize the bounding box on the original image\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def show_prediction(image, label, prediction):\n",
        "\n",
        "    fig,ax = plt.subplots(1, figsize=(10,10))\n",
        "    ax.imshow(image)\n",
        "    gt_boxes = np.argwhere(label[:,0]==0)\n",
        "    for match in gt_boxes:\n",
        "        anchor_box = label[match[0],-4:]\n",
        "        gt_box = label[match[0], -8:-4]\n",
        "        class_id = np.argwhere(label[match[0],:10]==1)\n",
        "\n",
        "        w = math.exp(gt_box[2]) * anchor_box[2]\n",
        "        h = math.exp(gt_box[3]) * anchor_box[3]\n",
        "        cx = gt_box[0] * anchor_box[2] + anchor_box[0]\n",
        "        cy = gt_box[1] * anchor_box[3] + anchor_box[1]\n",
        "\n",
        "        xmin = (cx - w/2) * IMG_SIZE\n",
        "        ymin = (cy - h/2) * IMG_SIZE\n",
        "        w = w * IMG_SIZE\n",
        "        h = h * IMG_SIZE\n",
        "\n",
        "        rect = patches.Rectangle((ymin,xmin),h, w,linewidth=1,edgecolor='g',facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(ymin+1, xmin+w-5, 'ground truth: ' + str(class_id[0,0]), color='g')\n",
        "\n",
        "    pred_boxes= np.argwhere(prediction[:,1] > 0)\n",
        "\n",
        "    for pred in pred_boxes:\n",
        "        box = prediction[pred[0],2:6]\n",
        "        class_id = int(prediction[pred[0],0])\n",
        "        prob = prediction[pred[0],1]\n",
        "        xmin = min(max(box[0],0),224)\n",
        "        ymin = min(max(box[1],0),224)\n",
        "        w = min(max(box[2],0),224) - xmin\n",
        "        h = min(max(box[3],0),224) - ymin\n",
        "        rect = patches.Rectangle((ymin,xmin),h, w,linewidth=1,edgecolor='r',facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(ymin+1, xmin+5, 'class: {}, prob: {:.2f}'.format(class_id, prob), color='r')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5ROAydmL_mn"
      },
      "outputs": [],
      "source": [
        "show_prediction(image[image_no],label[image_no],y_decoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ2pl0mBMBjV"
      },
      "outputs": [],
      "source": [
        "ssd_test_gen = SSDInputEncodingGenerator(IMG_SIZE,\n",
        "                 IMG_SIZE,\n",
        "                 layer_width=layer_width,\n",
        "                 n_classes=n_classes,\n",
        "                 num_boxes=num_boxes,\n",
        "                 s_max=s_max,\n",
        "                 s_min=s_min,\n",
        "                 aspect_ratio=aspect_ratio,\n",
        "                 pos_iou_threshold=pos_iou_threshold,\n",
        "                 neg_iou_threshold=neg_iou_threshold,\n",
        "                 background_id=0,\n",
        "                 images=x_test,\n",
        "                 labels=y_test,\n",
        "                data_size=test_size,\n",
        "                batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slkQCrLoMDiI"
      },
      "outputs": [],
      "source": [
        "test_image, test_label = next(iter(ssd_test_gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZZhDs0CMFkp"
      },
      "outputs": [],
      "source": [
        "test_pred = model.predict(test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4Oi7VVnMHjC"
      },
      "outputs": [],
      "source": [
        "image_no = 0\n",
        "test_decoded = decode_detections(np.expand_dims(test_pred[image_no], axis=0),\n",
        "                            n_classes=10,\n",
        "                              confidence_thresh=0.01,\n",
        "                              iou_threshold=0.45,\n",
        "                              top_k=100,\n",
        "                              img_height=IMG_SIZE,\n",
        "                              img_width=IMG_SIZE,\n",
        "                              background_id=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeTD6KrJMKPK"
      },
      "outputs": [],
      "source": [
        "show_prediction(test_image[image_no],test_label[image_no],test_decoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZrf2NF2MKVm"
      },
      "outputs": [],
      "source": [
        "# Function to dump prediction result in JSON format\n",
        "import json\n",
        "\n",
        "def dump_coco_json(dataset_size, batch_size, generator, model, out_file):\n",
        "\n",
        "    # Put the results in this list.\n",
        "    results = []\n",
        "    id_cnt = 0\n",
        "\n",
        "    for batch_X, batch_label in generator:\n",
        "        # Generate batch.\n",
        "        #batch_X, batch_label = next(generator)\n",
        "        # Predict.\n",
        "        y_pred = model.predict(batch_X)\n",
        "\n",
        "        # Decode.\n",
        "        y_pred = decode_detections(y_pred,\n",
        "                                   n_classes=10,\n",
        "                                   confidence_thresh=0.01,\n",
        "                                   iou_threshold=0.45,\n",
        "                                   top_k=200,\n",
        "                                   img_height=IMG_SIZE,\n",
        "                                  img_width=IMG_SIZE,\n",
        "                                  background_id=0)\n",
        "\n",
        "        # Convert each predicted box into the results format.\n",
        "        for k, batch_item in enumerate(y_pred):\n",
        "            for box in batch_item:\n",
        "                cat_id = box[0]\n",
        "                # Round the box coordinates to reduce the JSON file size.\n",
        "                xmin = float(round(box[2], 1))\n",
        "                ymin = float(round(box[3], 1))\n",
        "                xmax = float(round(box[4], 1))\n",
        "                ymax = float(round(box[5], 1))\n",
        "                width = xmax - xmin\n",
        "                height = ymax - ymin\n",
        "                bbox = [xmin, ymin, width, height]\n",
        "                result = {}\n",
        "                result['image_id'] = id_cnt\n",
        "                result['category_id'] = cat_id\n",
        "                result['score'] = float(round(box[1], 3))\n",
        "                result['bbox'] = bbox\n",
        "                results.append(result)\n",
        "            id_cnt += 1\n",
        "            if id_cnt == dataset_size:\n",
        "                break\n",
        "\n",
        "\n",
        "    with open(out_file, 'w') as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    print(\"Prediction results saved in '{}'\".format(out_file))\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgjuLpadMKbE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}